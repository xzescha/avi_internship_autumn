# Тестовое задание на стажировку Backend-разработчика в Авито

## Стек

* **Go** 1.25
* **PostgreSQL** 16
* **Docker / Docker Compose**
* **golangci-lint** для статического анализа
* **k6** для нагрузочного тестирования
* **testcontainers-go** (локально) / Docker Compose (в профиле `tests`) для E2E

---

## Особенности реализации и ручки

### Основное задание

#### Команды и пользователи

* `POST /team/add`
  Создаёт новую команду и её участников.

    * Если команда уже существует — возвращается ошибка `TEAM_EXISTS` (HTTP 400).
    * Пользователи создаются/обновляются (upsert) по `user_id`.
    * Хранится флаг активности пользователя `is_active` и принадлежность к команде.

* `GET /team/{team_name}`
  Возвращает состав команды и список активных/неактивных пользователей.

* `GET /team/{team_name}/users`
  Упрощённый список пользователей команды (например, для UI или дебага).

#### Pull Requestы

* `POST /pullRequest/create`
  Создаёт новый PR:

    * PR привязан к автору (`author_id`) и имеет статус `OPEN`.
    * Автоматически выбирает одного или нескольких ревьюверов из активных членов команды автора:
        * автор никогда не назначается ревьювером;
        * выбор идёт по простому балансирующему правилу (например, по статистике назначений или по случайному распределению).
    * Если PR с таким `pull_request_id` уже есть, то возвращается HTTP 409.

* `POST /pullRequest/reassign`
  Переназначает ревьювера у открытого PR:

    * снимает старого ревьювера;
    * ищет нового кандидата среди активных участников команды, исключая:
        * автора PR;
        * уже назначенных ревьюверов;
    * если заменить некем — возвращается бизнес-ошибка (409), PR остаётся с текущими ревьюверами.

* `POST /pullRequest/merge`
  Переводит PR в статус `MERGED`.
  После этого PR не участвует в логике назначения/переассайна, но остаётся в статистике.

### Дополнительные задания

#### 1. Статистика назначений

* `GET /stats/assignments`
  Дополнительный эндпоинт статистики. Возвращает:

  ```json
  {
    "by_reviewer": [
      { "user_id": "u2", "assignments": 5 },
      { "user_id": "u1", "assignments": 3 }
    ],
    "by_pr": [
      { "pull_request_id": "pr-1001", "assignments": 2 },
      { "pull_request_id": "pr-1002", "assignments": 1 }
    ]
  }
  ```

  Логика:

    * по ревьюверам: сколько раз каждый пользователь был назначен ревьювером;
    * по Pull Request: сколько ревьюверов назначено на каждый Pull Request.

#### 2. Массовая деактивация и безопасная переназначаемость

* `POST /users/bulkDeactivate`

  ```json
  {
    "team_name": "payments",
    "user_ids": ["u2", "u3"]
  }
  ```

  Делает сразу две вещи:

    1. Массово деактивирует указанных пользователей в рамках команды:

        * обновляет `is_active=false`;
        * возвращает, сколько пользователей реально изменило статус.

    2. Безопасно переназначает ревьюверов в открытых PR:

        * для всех открытых PR, где среди ревьюверов есть деактивированный пользователь:

            * деактивированных ревьюверов удаляет;
            * пытается подобрать замену среди активных участников команды: (isActive == true)
                * автор PR не может быть назначен;
                * уже назначенные ревьюверы не дублируются.
        * если заменить некем — PR остаётся с сокращённым списком ревьюверов.
        * возвращается количество затронутых PR.

  Логика оптимизирована под умеренный объём данных из условия (до 20 команд и 200 пользователей) и укладывается в целевой SLA (примерно 100 мс на операцию в этих масштабах).

---

## Конфигурация и окружение

Все параметры задаются через переменные окружения (и `.env`):

```env
HTTP_PORT=8080

DB_HOST=localhost        # или postgres_PR_db в docker-compose
DB_PORT=5432
DB_USER=admin_PR
DB_PASSWORD=appPass_QWERTY
DB_NAME=PR_serv
DB_SSLMODE=disable
```

В коде есть дефолты:

```go
defaultDBUser     = "admin_PR"
defaultDBPassword = "appPass_QWERTY"
defaultDBName     = "PR_serv"
defaultDBSSLMode  = "disable"
```

Если переменная не задана — используется дефолт.
Не стал добавлять файл .env, делать подстановку переменных для удобства проверки.
Логично, что на реальном проекте надо использовать .env и не допускать попадания ключей и паролей в git

---

## Тестирование

### E2E-тестирование

E2E-тест лежит в `test/e2e/e2e_test.go` и помечен тегом `//go:build e2e`.

Что делает `TestE2E_FullFlow`:

1. Поднимает PostgreSQL:

    * локально через `testcontainers-go`;
    * в Docker Compose — использует уже запущенный сервис `postgres` по DSN из `E2E_DSN`.
2. Применяет SQL-миграции (в локальном режиме).
3. Собирает репозитории и сервисы, поднимает HTTP-роутер в `httptest.Server`.
4. Прогоняет сценарий:

    * `POST /team/add` — создаёт тестовую команду;
    * `POST /pullRequest/create` — создаёт PR и назначает ревьюверов;
    * `POST /users/bulkDeactivate` — деактивирует часть пользователей и пересобирает ревьюверов;
    * `GET /stats/assignments` — проверяет, что статистика отрабатывает без ошибок.

#### E2E в Docker Compose

В `docker-compose.yml` есть профиль `tests`:

* сервис `e2e`:

    * использует билд-таргет `builder`;
    * получает `E2E_DSN=postgres://...@postgres:5432/PR_serv?sslmode=disable`;
    * запускает `go test -tags=e2e ./test/e2e -v`.

Поднять и прогнать E2E можно так:

```bash
make up-tests
```

(в коде вызов команды `docker compose --profile tests up --build`).

### Нагрузочное тестирование (k6)

Сценарий лежит в `test/load/k6_pr_scenario.js`.

#### Сценарий

* В `setup` создаётся 20 команд по 10 пользователей: суммарно 200 пользователей (как в условии).
* Используется `constant-arrival-rate`:

    * **5 бизнес-итераций в секунду**;
    * каждая итерация делает:

        * `POST /pullRequest/create`;
        * (опционально) `POST /pullRequest/reassign`;
        * раз в несколько итераций — `GET /stats/assignments`.
* Длительность теста: 60 секунд.

#### SLI и кастомная метрика

* По времени:

    * `http_req_duration`: порог `p(95) < 300ms`.
* По успешности:

    * введена кастомная метрика `biz_fail_rate`:

        * считает ошибкой только:

            * любые 5xx;
            * неожиданные статусы (кроме бизнес-409/400/404, которые допустимы).
    * порог `biz_fail_rate < 0.001` => успешность **>= 99.9%**.

#### Результаты (пример из моего последнего прогона)

* `http_req_duration p(95) примерно 6.9 ms` (требование <= 300 ms выполнено с большим запасом).
* `biz_fail_rate = 0%` (ни одной бизнес-ошибки при тесте).
* примерно 11 HTTP-запросов/сек (примерно 5 бизнес-итераций/сек).

Запуск k6 в Docker Compose:

* в профиле `tests` есть сервис `k6`, который:

    * стартует после успешного `e2e`;
    * бьётся по `BASE_URL=http://app:8080`;
    * монтирует `./test/load:/scripts`.

Всё это запускается той же командой:

```bash
make up-tests
```

(после завершения тестов контейнеры можно остановить `make down`).

---

## Линтер

Используется **golangci-lint** (v2) в Docker-образе `golangci/golangci-lint`.
Конфигурация в `.golangci.yml`:

Включены линтеры:

* `govet` — стандартный анализатор от Go;
* `staticcheck`, `gosimple` — «умные» проверки и упрощения;
* `ineffassign` — неиспользуемые присваивания;
* `errcheck` — проверка ошибок;
* `revive` — стиль и best practices;
* `gocyclo` — цикломатическая сложность;
* `goconst` — поиск магических констант;
* `misspell` — орфография.

Отключены:

* `funlen` — не ругаемся на длину функций;
* `dupl` — не ругаемся на дубликаты, чтобы не мешать тестам и репозиториям (однако в самой бизнес-логике декомпозиция проведена);
* `nakedret` — допускаются naked returnы (чтобы не оставлять unhandled errors в некоторых местах).

Отдельно стоит отметить:

* `revive` заставляет комментировать все экспортируемые типы/функции/пакеты.

    * В результате у доменных моделей, репозиториев, сервисов и HTTP-хендлеров есть короткие и понятные комментарии.
    * Это улучшает читабельность и делает публичный API сервиса более очевидным.

Запуск линтера:

```bash
make lint
```

( в коде команда `docker run golangci/golangci-lint ...` с монтированием проекта).

---

## Makefile

Все основные команды по запуску и проверкам вынесены в `Makefile`:

* `make up` - поднять сервис (postgres + app) через Docker Compose.
* `make up-tests` - запустить профиль `tests`: E2E + k6.
* `make down` - остановить и убрать контейнеры.
* `make test` - `go test ./...`.
* `make test-e2e-local` - локальный запуск E2E (`go test -tags=e2e ./test/e2e`).
* `make lint` - прогон golangci-lint через Docker.
* `make build` / `make run` - локальная сборка и запуск бинарника без Docker.

### Как достигается заданная скорость ответа (SLI по времени). По сути с чем я столкнулся и как это решил

Требование из задания: при «умеренном объёме данных» (до 20 команд и до 200 пользователей) сервис должен укладываться в SLI по времени ответа **<= 300 мс**.

Фактически в нагрузочном тесте (20 команд по 10 пользователей, примерно 5 бизнес-операций/с) получилось:

- `http_req_duration p(95): 6.9 ms`
- средняя задержка: 4–5 ms

Основные причины, почему сервис укладывается в заданный SLI с большим запасом:

1. **Простая бизнес-логика**
    - Все основные операции (создание PR, reassignment, статистика, bulkDeactivate) работают с небольшим количеством данных - максимум сотни пользователей и ограниченным числом открытых PR.
    - Логика назначения и переназначения ревьюверов не делает «глубоких» обходов по БД, вся фильтрация кандидатов происходит в памяти над уже полученным списком пользователей команды.

2. **Ограничение объёма данных по условию**
    - Весь расчёт проектировался под объём: **<= 20 команд, <= 200 пользователей**.
    - В этом диапазоне операции по выбору ревьюверов и пересборке списков PR имеют по сути линейную сложность по размеру команды (O(n) для n <= 10–100), что даёт малую задержку даже при последовательной обработке.

3. **Минимизация количества запросов к БД**
    - Для операций типа `bulkDeactivate` используется:
        - один массовый апдейт для деактивации пользователей команды;
        - выборка нужных открытых PR и ревьюверов агрегированными запросами.
    - Никаких N+1-запросов на уровне бизнес-логики — все данные для перераспределения ревьюверов подтягиваются пачками, а затем обрабатываются в памяти.

4. **Индексы и схема БД**
    - В миграциях добавлены индексы по ключевым полям:
        - `users(team_name, is_active)`,
        - внешние ключи для связей `PR -> users`, `assignments -> PR/user`.
    - Это ускоряет выборку активных пользователей команды и поиск назначений по PR/ревьюверу, которые используются в эндпоинтах `create`, `reassign`, `bulkDeactivate` и `stats`.

5. **Размещение сервиса и БД в одной Docker-сети**
    - Приложение и PostgreSQL работают в одной docker-сети, общаются по внутреннему адресу `postgres:5432`.
    - Это резко снижает сетевые накладные расходы и даёт стабильную низкую сетевую задержку.

6. **Отсутствие тяжёлых операций**
    - В обработчиках нет тяжёлого логирования, синхронных внешних HTTP-запросов, сериализации больших структур и т.п.
    - Все ответы — небольшие по размеру JSON-структуры, что упрощает маршалинг/анмаршалинг и не создаёт давления на GC.

В результате даже при искусственно зажатых условиях (постоянный RPS: 5 бизнес-итераций/с и до 11 HTTP-запросов/с) сервис остаётся заметно ниже требуемого порога в 300 мс, что подтверждается результатами нагрузочного.

## Прогон нагрузочного тестирования (результат)

```bash
k6-1          |   █ THRESHOLDS 
k6-1          | 
k6-1          |     biz_fail_rate
k6-1          |     ✓ 'rate<0.001' rate=0.00%
k6-1          | 
k6-1          |     http_req_duration
k6-1          |     ✓ 'p(95)<300' p(95)=6.71ms
k6-1          | 
k6-1          | 
k6-1          |   █ TOTAL RESULTS 
k6-1          | 
k6-1          |     checks_total.......: 680     11.32159/s
k6-1          |     checks_succeeded...: 100.00% 680 out of 680
k6-1          |     checks_failed......: 0.00%   0 out of 680
k6-1          | 
k6-1          |     ✓ team created or already exists
k6-1          |     ✓ create PR 201 or conflict
k6-1          |     ✓ reassign ok or business-conflict
k6-1          |     ✓ stats 200
k6-1          | 
k6-1          |     CUSTOM
k6-1          |     biz_fail_rate..................: 0.00%  0 out of 680
k6-1          | 
k6-1          |     HTTP
k6-1          |     http_req_duration..............: avg=4.26ms   min=419.79µs med=3.47ms   max=22.36ms  p(90)=6.41ms   p(95)=6.71ms  
k6-1          |       { expected_response:true }...: avg=4.26ms   min=419.79µs med=3.47ms   max=22.36ms  p(90)=6.41ms   p(95)=6.71ms  
k6-1          |     http_req_failed................: 0.00%  0 out of 680
k6-1          |     http_reqs......................: 680    11.32159/s
k6-1          | 
k6-1          |     EXECUTION
k6-1          |     iteration_duration.............: avg=113.57ms min=104.69ms med=113.64ms max=132.43ms p(90)=116.44ms p(95)=117.38ms
k6-1          |     iterations.....................: 300    4.994819/s
k6-1          |     vus............................: 0      min=0        max=0 
k6-1          |     vus_max........................: 10     min=10       max=10
k6-1          | 
k6-1          |     NETWORK
k6-1          |     data_received..................: 806 kB 13 kB/s
k6-1          |     data_sent......................: 140 kB 2.3 kB/s
k6-1          | 
k6-1          | 
k6-1          | 
k6-1          | 
k6-1          | running (1m00.1s), 00/10 VUs, 300 complete and 0 interrupted iterations
k6-1          | pr_flow ✓ [ 100% ] 00/10 VUs  1m0s  5.00 iters/s
```